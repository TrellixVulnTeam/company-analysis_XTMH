{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import requests, sys\n",
    "from bs4 import BeautifulSoup\n",
    "# from xbrl import XBRLParser, GAAP, GAAPSerializer\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Method that extracts SPX data from the Wikipedia page on the list of S&P 500 companies\n",
    "    @param: None\n",
    "    @return: a Pandas dataframe with columns for the (1) ticker, (2) security, \n",
    "    (3) sector and (4) CIK ID [to search SEC Edgar]\n",
    "'''\n",
    "def getSPXTickers():\n",
    "    URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    res = requests.get(URL).text\n",
    "    soup = BeautifulSoup(res, 'lxml')\n",
    "    \n",
    "    # Generates a CIK set to only add unique companies with a unique CIK ID\n",
    "    # This is important since several companies might have different share classes\n",
    "    CIKS = set()\n",
    "    \n",
    "    # Sets up the SPX dictionary to store the values from each company\n",
    "    spx_dict = {}\n",
    "    spx_dict['ticker'] = [] ; spx_dict['security'] = [] \n",
    "    spx_dict['sector'] = [] ; spx_dict['cik'] = []\n",
    "    \n",
    "    # Loops through the first 506 (505) table rows on the Wikipedia page\n",
    "    # 5 companies have different share classes available to investors\n",
    "    for items in soup.find('table', class_='wikitable').find_all('tr')[1:506]:\n",
    "        data = items.find_all(['td'])\n",
    "        if data[7] not in CIKS:\n",
    "            CIKS.add(data[7])\n",
    "            \n",
    "            # This little bit of trickery accounts for the edge-case of the security 'BF.B' \n",
    "            # which should be 'BF-B'\n",
    "            if '.' in data[0].text:\n",
    "                spx_dict['ticker'].append(data[0].text.strip('\\n').replace('.','-'))\n",
    "            else:\n",
    "                spx_dict['ticker'].append(data[0].text.strip('\\n'))\n",
    "                \n",
    "            spx_dict['security'].append(data[1].text.strip('\\n'))\n",
    "            spx_dict['sector'].append(data[3].text.strip('\\n'))\n",
    "            spx_dict['cik'].append(data[7].text.strip('\\n'))\n",
    "    return pd.DataFrame.from_dict(spx_dict)\n",
    "\n",
    "# Code to execute below:\n",
    "spx_df = getSPXTickers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note for this method: 'institutional holders error' comes from a try-except in Line 283 of yfinance base.py\n",
    "# This try-except was added \n",
    "\n",
    "'''\n",
    "Method that stores information on each SPX stock for further processing\n",
    "    @param: a Pandas Core Series with each ticker\n",
    "    @param: a list of each field to be extracted from the 'security.info' dictionary\n",
    "    @return: a Pandas dataframe with columns for the (1) ticker, and (2-X) for each extracted\n",
    "    field from the 2nd param\n",
    "'''\n",
    "def storeSPXParams(tickers, fields):\n",
    "    spxParams_dict = {}\n",
    "    spxParams_dict['ticker'] = []\n",
    "    for field in fields:\n",
    "        spxParams_dict[field] = []\n",
    "    \n",
    "    # Loops through all tickers and populates the spxParams dictionary\n",
    "    for ticker in tickers:\n",
    "        yf_ticker = yf.Ticker(ticker)\n",
    "        spxParams_dict['ticker'].append(ticker)\n",
    "        for field in fields:\n",
    "            try:\n",
    "                spxParams_dict[field].append(yf_ticker.info[field])\n",
    "            except:\n",
    "                spxParams_dict[field].append(-1)\n",
    "                print(\"{} failed for {}\".format(str(field), ticker))\n",
    "    return pd.DataFrame.from_dict(spxParams_dict)\n",
    "\n",
    "# Code to execute below:\n",
    "fields = ['beta', 'trailingPE', 'marketCap', 'priceToSalesTrailing12Months', \n",
    "         'enterpriseToEbitda', 'bookValue', 'previousClose', 'sharesShort']\n",
    "spx_params = storeSPXParams(spx_df.ticker,fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker                                 ALB\n",
       "beta                               1.46429\n",
       "trailingPE                         22.4302\n",
       "marketCap                       9826415616\n",
       "priceToSalesTrailing12Months       2.91135\n",
       "enterpriseToEbitda                  14.636\n",
       "bookValue                           37.846\n",
       "previousClose                         91.6\n",
       "sharesShort                       12186827\n",
       "Name: 15, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Method to populate a dictionary \n",
    "'''\n",
    "def get10KEdgarParams(CIK, params, start_year=2012, end_year=2019):\n",
    "    # Step 1: Navigate to the SEC search results page for the given company's 10-K filings\n",
    "    URL = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type=10-K&dateb=&owner=exclude&count=100\".format(str(CIK))\n",
    "    res = requests.get(URL).text\n",
    "    soup = BeautifulSoup(res, 'lxml')\n",
    "    step1rows = soup.find('table', class_='tableFile2').find_all('tr')[:100]\n",
    "    \n",
    "    # Loops through until we reach the end year - we descend from the end year ot the start year\n",
    "    index = 1\n",
    "    while (step1rows[index].find_all('td')[3].text[:4] != str(end_year + 1) or step1rows[index].find_all('td')[0].text != '10-K'):\n",
    "        index += 1\n",
    "        \n",
    "    # Step 2: For each year from the end year to the start year, relevant params are extracted and populated\n",
    "    \n",
    "    # Populates dataframe (columns = parameters, index = years)\n",
    "    years = range(start_year, end_year + 1)\n",
    "    params_df = pd.DataFrame(columns=params, index=years)\n",
    "    cur_year = end_year # this is an int\n",
    "    \n",
    "    while (step1rows[index].find_all('td')[3].text[:4] != str(start_year)):\n",
    "        if index > (len(step1rows) - 1):\n",
    "            break\n",
    "        if step1rows[index].find_all('td')[0].text != '10-K':\n",
    "            index += 1\n",
    "            pass\n",
    "        else:\n",
    "            print(step1rows[index].find_all('td')[3].text[:4])\n",
    "            url_ext = step1rows[index].find_all('td')[1].find_all('a')[0]['href']\n",
    "            URL2 = \"https://www.sec.gov{}\".format(str(url_ext))\n",
    "            print(URL2)\n",
    "            res = requests.get(URL2).text\n",
    "            soup = BeautifulSoup(res, 'html.parser')\n",
    "            \n",
    "            try:\n",
    "                table_tags = soup.find('table', class_='tableFile', summary='Data Files')\n",
    "            except:\n",
    "                print(\"XML XBRL Tables not found\")\n",
    "                break\n",
    "                \n",
    "            # Step 3: Parses through the different table tags to find the XBRL Instance XML Document\n",
    "            table_cells = table_tags.find_all('td')\n",
    "            for i in range(len(table_cells)):\n",
    "                if 'INS' in table_cells[i].text:\n",
    "                    try:\n",
    "                        doc_ext = table_cells[i+1].a['href']\n",
    "                        break\n",
    "                    except:\n",
    "                        print(\"Alternative table format\")\n",
    "                    try:\n",
    "                        doc_ext = table_cells[i-1].a['href']\n",
    "                        break\n",
    "                    except:\n",
    "                        print(\"No table format found\")\n",
    "                        break\n",
    "                        \n",
    "            # Step 4: Generates the URL for the 10-K file\n",
    "            URL3 = \"https://www.sec.gov{}\".format(str(doc_ext))\n",
    "            res = requests.get(URL3).text\n",
    "            soup = BeautifulSoup(res, 'lxml')\n",
    "            \n",
    "            # Step 4: Loops through all the parameters to find them in the 10-K XML File\n",
    "            searchable_file = soup.find('html')\n",
    "            for param in params:\n",
    "                try:\n",
    "#                     for v in searchable_file.find_all(param):\n",
    "#                         print(v['contextref'])\n",
    "                    params_df[param][cur_year] = searchable_file.find(param).text #['contextref']\n",
    "                except:\n",
    "                    print(\"Parameter {} not found\".format(str(param)))\n",
    "            \n",
    "            # Step 5: Update relevant parameters before looping\n",
    "            cur_year -= 1\n",
    "            index += 1\n",
    "    \n",
    "    return params_df      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<us-gaap:revenuefromcontractwithcustomerexcludingassessedtax contextref=\"FD2019Q4YTD_srt_ProductOrServiceAxis_us-gaap_ProductMember\" decimals=\"-3\" id=\"d73780586e950-wk-Fact-EC9892A4D0F9B78AFD46CC7273422894\" unitref=\"usd\">2021150000</us-gaap:revenuefromcontractwithcustomerexcludingassessedtax>\n",
      "<us-gaap:revenues contextref=\"P01_01_2019To03_31_2019\" decimals=\"-5\" unitref=\"Unit_USD\">1990600000</us-gaap:revenues>\n",
      "<us-gaap:revenues contextref=\"FD2019Q4YTD\" decimals=\"-5\" id=\"d52406991e1444-wk-Fact-01F249567922690948FBCD15C8AABD00\" unitref=\"usd\">10086800000</us-gaap:revenues>\n",
      "<us-gaap:revenues contextref=\"FROM_Jan01_2019_TO_Dec31_2019_Entity_0000732717_srt_ProductOrServiceAxis_t_ServiceAndOtherMember\" decimals=\"-6\" id=\"ID_2201\" unitref=\"USD\">163499000000</us-gaap:revenues>\n",
      "<us-gaap:revenues contextref=\"FD2019Q4YTD_srt_ConsolidationItemsAxis_us-gaap_OperatingSegmentsMember_us-gaap_StatementBusinessSegmentsAxis_ato_DistributionSegmentMember\" decimals=\"-3\" id=\"d8306332e928-wk-Fact-58AE71FF44AD334A0B289162D0F9485A\" unitref=\"usd\">2745461000</us-gaap:revenues>\n",
      "<us-gaap:revenuefromcontractwithcustomerexcludingassessedtax contextref=\"FD2020Q4YTD_srt_ProductOrServiceAxis_us-gaap_SubscriptionAndCirculationMember\" decimals=\"-5\" id=\"d14110000e1029-wk-Fact-437FE669308FEB6A15CAF5B8FED6BDE3\" unitref=\"usd\">2751900000</us-gaap:revenuefromcontractwithcustomerexcludingassessedtax>\n",
      "<us-gaap:revenues contextref=\"FD2017Q4YTD\" decimals=\"-5\" id=\"Fact-715B7F00B92B066C8B18D71CF83F86A5\" unitref=\"usd\">12372000000</us-gaap:revenues>\n",
      "<us-gaap:revenues contextref=\"P08_26_2018To08_31_2019\" decimals=\"-3\" id=\"Factid_7198367\" unitref=\"Unit_USD\">11863743000</us-gaap:revenues>\n",
      "<us-gaap:revenues contextref=\"FD2019Q4YTD\" decimals=\"-3\" id=\"d36860014e1120-wk-Fact-081F81C7150F522C816F9B90FDA5F930\" unitref=\"usd\">2324626000</us-gaap:revenues>\n",
      "<us-gaap:revenuefromcontractwithcustomerexcludingassessedtax contextref=\"Duration_12_30_2018_To_12_28_2019_WHZP1tF_mEGI6X3piPGhOw\" decimals=\"-5\" id=\"Tc_cL-ckuCWwEemqhEQVwSZ4Q_2_3\" unitref=\"Unit_Standard_USD_7YkhMLgSRUCMOA9xBE9Trg\">7070100000</us-gaap:revenuefromcontractwithcustomerexcludingassessedtax>\n"
     ]
    }
   ],
   "source": [
    "for i in range(50,60):\n",
    "    cik = spx_df.cik[i]\n",
    "    URL = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type=10-K&dateb=&owner=exclude&count=100\".format(str(cik))\n",
    "    res = requests.get(URL).text\n",
    "    soup = BeautifulSoup(res, 'lxml')\n",
    "    #     print(URL)\n",
    "    x = soup.find('table', class_='tableFile2').find_all('tr')[:100]\n",
    "    url_ext = x[1].find_all('td')[1].find_all('a')[0]['href']\n",
    "    # print(url_ext)\n",
    "    URL2 = \"https://www.sec.gov{}\".format(str(url_ext))\n",
    "    res = requests.get(URL2).text\n",
    "    soup = BeautifulSoup(res, 'html.parser')\n",
    "    table_tags = soup.find('table', class_='tableFile', summary='Data Files')\n",
    "    next_bool = False \n",
    "    table_cells = table_tags.find_all('td')\n",
    "    for i in range(len(table_cells)):\n",
    "        if 'INS' in table_cells[i].text:\n",
    "            try:\n",
    "                doc_ext = table_cells[i+1].a['href']\n",
    "                break\n",
    "            except:\n",
    "                print(\"Alternative table format\")\n",
    "            try:\n",
    "                doc_ext = table_cells[i-1].a['href']\n",
    "                break\n",
    "            except:\n",
    "                print(\"No table format found\")\n",
    "                break\n",
    "    # print(doc_ext)\n",
    "    URL3 = \"https://www.sec.gov{}\".format(doc_ext)\n",
    "    res = requests.get(URL3).text\n",
    "    soup = BeautifulSoup(res, 'lxml')\n",
    "    searchable = soup.find('html')\n",
    "    x = searchable.find_all('us-gaap:revenues')\n",
    "    if not x:\n",
    "        x = searchable.find_all('us-gaap:revenuefromcontractwithcustomerexcludingassessedtax')\n",
    "    if not x:\n",
    "        x = searchable.find_all('us-gaap:regulatedandunregulatedoperatingrevenue')\n",
    "    print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000009389'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spx_df.cik[61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.sec.gov/Archives/edgar/data/1065088/000120677420001423/0001206774-20-001423-index.htm'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "https://www.sec.gov/Archives/edgar/data/764180/000076418020000018/0000764180-20-000018-index.htm\n",
      "2019\n",
      "https://www.sec.gov/Archives/edgar/data/764180/000076418019000023/0000764180-19-000023-index.htm\n",
      "2018\n",
      "https://www.sec.gov/Archives/edgar/data/764180/000076418018000028/0000764180-18-000028-index.htm\n",
      "2017\n",
      "https://www.sec.gov/Archives/edgar/data/764180/000076418017000028/0000764180-17-000028-index.htm\n",
      "2016\n",
      "https://www.sec.gov/Archives/edgar/data/764180/000076418016000128/0000764180-16-000128-index.htm\n",
      "2015\n",
      "https://www.sec.gov/Archives/edgar/data/764180/000076418015000022/0000764180-15-000022-index.htm\n",
      "2014\n",
      "https://www.sec.gov/Archives/edgar/data/764180/000076418014000025/0000764180-14-000025-index.htm\n",
      "2013\n",
      "https://www.sec.gov/Archives/edgar/data/764180/000076418013000024/0000764180-13-000024-index.htm\n",
      "Parameter us-gaap:netincomeloss not found\n"
     ]
    }
   ],
   "source": [
    "# ans = get10KEdgarParams(spx_df.cik[34], ['us-gaap:netincomelossavailabletocommonstockholdersbasic'], start_year=2012, end_year=2019)\n",
    "ans = get10KEdgarParams(spx_df.cik[24], ['us-gaap:netincomeloss'], start_year=2012, end_year=2019)\n",
    "# ans = get10KEdgarParams(spx_df.cik[55], ['us-gaap:profitloss'], start_year=2012, end_year=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>us-gaap:netincomeloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>3390000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>4180000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>4535000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>5070000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>5241000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>14239000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>-1293000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     us-gaap:netincomeloss\n",
       "2012                   NaN\n",
       "2013            3390000000\n",
       "2014            4180000000\n",
       "2015            4535000000\n",
       "2016            5070000000\n",
       "2017            5241000000\n",
       "2018           14239000000\n",
       "2019           -1293000000"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0001065088'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spx_df.cik[162]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SECParser",
   "language": "python",
   "name": "secparser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
